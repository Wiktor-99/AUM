{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projekt Algorytmy Uczenia Maszynowego\n",
    "\n",
    "## Cel Projektu\n",
    "Celem tego projektu jest wykorzystanie algorytmów uczenia maszynowego do zrealizowania zadania klasyfikacji.\n",
    "## Przedstawienie problemu\n",
    "W ramach projektu zostanie wykonane zadanie klasyfikacji zdjęć zwierząt. Przygotowana implementacja powinna rozpoznawać \n",
    "zwierzęta z pięciu różnych klas, w tym przypadku będą to:\n",
    "* kurczak\n",
    "* owca\n",
    "* koń \n",
    "* pająk\n",
    "* słoń\n",
    "\n",
    "Do realizacji zadania wykorzystano zbiór danych dostępny pod linkiem:\n",
    "https://www.kaggle.com/datasets/alessiocorrado99/animals10\n",
    "## Wybrane algorytmy uczenia maszynowego\n",
    "Jednym z założeń projektowych jest zrealizowanie danego zadnia z wykorzystaniem trzech podstawowych \n",
    "algorytmów uczenia maszynowego, w tym przypadku zdecydowano się na wybranie następujących algorytmów:\n",
    "* Maszyna wektorów nośnych\n",
    "* Wielowarstwowy perceptron \n",
    "* K najbliższych sąsiadów \n",
    "\n",
    "Po wykorzystaniu algorytmów uczenia maszynowego należało zaimplementować mechanizm łączenia wykorzystywanych algorytmów.\n",
    "W tym projekcie zdecydowano się na wykorzystanie `głosowania większościowego`.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie oraz wizualizacja zbioru danych  \n",
    "Jednym z kluczowych elementów w zadania klasyfikacji jest dobranie odpowiedniego zbioru danych.\n",
    "W tym przypadku zaszła potrzeba usunięcia części elementów ze zbioru ze względu na ich jakość.\n",
    "Zdjęcia w których występowało wiele zwierząt, były bardzo zaszumione albo przedstawiały nie te \n",
    "zwierzęta musiały zostać usunięte. W efekcie końcowym zbiory danych zostały zmniejszone do 250 \n",
    "zdjęć dla każdej klasy. Mimo zmniejszenia zbioru uczącego uzyskano lepsze rezultaty.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imshow, imread_collection\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_images(class_name, values, labels):\n",
    "    path_to_image = 'raw-img/'\n",
    "    postfix = '/*.jpeg'\n",
    "    image_size = (128,128)\n",
    "    for image in imread_collection(path_to_image + class_name + postfix):\n",
    "        values.append(resize(image, image_size))\n",
    "        labels.append(class_name)\n",
    "\n",
    "def create_data_set():\n",
    "    class_names = ['spider', 'horse', 'elephant', 'chicken', 'sheep']\n",
    "    values = []\n",
    "    labels = []\n",
    "    for class_name in class_names:\n",
    "        load_images(class_name, values, labels)\n",
    "\n",
    "    return np.array(values), np.array(labels)\n",
    "\n",
    "def plot_data_set(x, y, unique_labels):\n",
    "    fig, axes = plt.subplots(1, len(unique_labels))\n",
    "    fig.set_size_inches(15,4)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for ax, label in zip(axes, unique_labels):\n",
    "        ax.imshow(x[np.where(y == label)[0][0]])\n",
    "        ax.axis('off')\n",
    "        ax.set_title(label)\n",
    "\n",
    "x, y = create_data_set()\n",
    "unique_labels = np.unique(y)\n",
    "\n",
    "plot_data_set(x, y, unique_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ekstrakcja cech\n",
    "\n",
    "Niezbędnym elementem w wykorzystaniu algorytmów uczenia maszynowego do zadania klasyfikacji jest wyciągniecie \n",
    "najważniejszych informacji ze zbioru uczącego. W przypadku danych jakimi są obrazy można zrobić to na wiele sposobów, \n",
    "miedzy innymi:\n",
    "* wykorzystać histogram\n",
    "* wykorzystać operatory pozwalające uwypuklić krawędzie\n",
    "* wykorzystać bardziej skomplikowane algorytmy takie jakie np. transformacja HOGa czy cechy Haar\n",
    "\n",
    "Do realizacji tego zadania zdecydowano się na wybranie transformaty `HOG`.\n",
    "\n",
    "### Transformacja HOG(Histogram zorientowanych gradientów)\n",
    "\n",
    "Opis cech wykorzystujący histogram zorientowanych gradientów, wymaga wykonania wielu pośrednich operacji \n",
    "aby uzyskiwać oczekiwane rezultaty. \n",
    "\n",
    "##### Wyznaczenie gradientów\n",
    "\n",
    "Pierwszym krokiem jest wyznaczanie gradientów z w pionie oraz w poziomie wykorzystując filtrując z wykorzystaniem jąder:\n",
    "\n",
    "$$ [-1, 0, 1] \\text{ oraz }  [-1, 0, 1]^T$$\n",
    "\n",
    "##### Kierunek oraz natężenie gradientów\n",
    "\n",
    "Kolejnym krokiem jest obliczenie natężenie gradientów oraz ich kierunków zgodnie z wzorami\n",
    "\n",
    "\n",
    "$$\n",
    "    g = \\sqrt{g^2_x + g^2_y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\theta = arctan\\frac{g_y}{g_x}\n",
    "$$\n",
    "\n",
    "#### Obliczanie histogramów gradientów\n",
    "\n",
    "Kolejnym elementem było obliczanie gradientów, aby to zrobić obraz należy podzielić na komórki a następnie dla każdej \n",
    "z komórek dodać wartości gradientów aby uzyskać histogram dla danej ilości kierunków.\n",
    "\n",
    "#### Normalizacja gradientów\n",
    "\n",
    "Ostatnim elementem jest normalizacja gradientów. Normalizacja gradientów polega na dobraniu bloków złożonych z komórek\n",
    "a następnie wykonania dla niej normalizacji w taki sposób jak dla wektorów.\n",
    "\n",
    "\n",
    "### Wykorzystanie transformacji HOG\n",
    "\n",
    "W ramach projektu wykorzystano gotową implementację pozwalającą z biblioteki scikt-image na dostarczenie deskryptorów HOG.\n",
    "Przy czym najważniejsze parametry jakie zostały wykorzystane to:\n",
    "\n",
    "* orientation — ilość kierunków gradientów branych pod uwagę\n",
    "* pixels_per_cell — ilość pikseli w jednej komórce \n",
    "* cells_per_block — ilość komórek w jednym bloku\n",
    "\n",
    "Wyżej wymienione parametry zostały dobrane z wykorzystaniem przeszukania (`GridSearchCV`), tak aby dostarczyć\n",
    "klasyfikatorom najlepszy zestaw cech.\n",
    "\n",
    "Aby przedstawić efekt działania transformaty HOG można wykorzystać parameter `visualize`, dzięki któremu można zwrócić\n",
    "wizualny efekt działania operacji co zostało przedstawione poniżej.\n",
    "\n",
    "\n",
    "Ostatnim elementem poniższego bloku jest podzielenie zbioru(z wykorzystaniem funkcji `train_test_split`) na część testową\n",
    "oraz część treningową w proporcji jeden do cztery. Przy podziale wykorzystano parameter `shuffle`, który pozwala na\n",
    "wstępne przetasowanie danych. Wykorzystano także parametr `random_state`, który pozwala na podział danych \n",
    "w powtarzalny sposób.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "\n",
    "def extract_hog_features(values, orientation = 11, pixels_per_cell=(8,8), cells_per_block=(7,7)):\n",
    "    scalify = StandardScaler()\n",
    "    return scalify.fit_transform(\n",
    "        np.array([ hog(img, orientations=orientation, pixels_per_cell=pixels_per_cell, cells_per_block=cells_per_block, channel_axis=-1) for img in values ]))\n",
    "\n",
    "def plot_hog_features_extraction(x, y, unique_labels, orientation = 9, pixels_per_cell=(9,9), cells_per_block=(4,4)):\n",
    "    fig, axes = plt.subplots(1, len(unique_labels))\n",
    "    fig.set_size_inches(15,4)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for ax, label in zip(axes, unique_labels):\n",
    "        fd, img = hog(\n",
    "            image=x[np.where(y == label)[0][0]],\n",
    "            orientations=orientation,\n",
    "            pixels_per_cell=pixels_per_cell,\n",
    "            cells_per_block=cells_per_block,\n",
    "            visualize=True,\n",
    "            channel_axis=-1) \n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(label)\n",
    "\n",
    "hog_train = extract_hog_features(x)\n",
    "plot_hog_features_extraction(x, y, unique_labels)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(hog_train, y, test_size=0.2, shuffle=True, random_state=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystanie maszyny wektorów nośnych(SVM) w zadaniu klasyfikacji\n",
    "\n",
    "Algorytm `SVM` można potraktować jako rozwinięcie modelu perceptronu. Wykorzystanie algorytmu perceptronu daje możliwość minimalizowania błędu klasyfikacji, z kolei podstawowym celem SVM jest maksymalizacja marginesu. Przy czym margines jest odległością pomiędzy hiperprzestrzenią rozdzielającą(granicą decyzyjną) a najbliższymi próbkami uczącymi(`wektorami nośnymi`). Wykorzystując algorytm SVM dążymy do uzyskania szerokich granic decyzyjnych, ponieważ takie modele są bardziej odporne na błędy uogólnienia, natomiast możliwie jest wystąpienie przetrenowanie. \n",
    "\n",
    "W przypadku algorytmu ważnym parametrem jest  `C`, sterując wartością `C` można kontrolować karę za niewłaściwą klasyfikację. Duże wartości `C` zwiększają kary za błędną klasyfikację, natomiast małe wartości `C` zmniejszają kary. Im większy jest parametr `C` tym bardziej generalizujmy model jednocześnie zmniejszamy margines, małe wartości `C` mogą doprowadzić do przetrenowania modelu. Wartość parametru `C` została dobrana za wykorzystaniem przeszukania GridSearchCV.\n",
    "\n",
    "Kolejnym istotnym elementem algorytmu `SVM` jest dobranie odpowiedniego jądra. Zadanie klasyfikacji dla danych rozdzielnych z wykorzystaniem liniowej hiperpłaszczyzny jest trywialnym zadaniem z punktu widzenia algorytmu `SVM`. W momencie w którym dane nie są rozdzielne w sposób liniowy sytuacja jest gorsza, jednak zastosowanie odpowiedniej funkcji jądrowej pozwala na utworzenie nieliniowych kombinacji pierwotnych cech, które finalnie mogą być mapowane na przestrzenie o większej liczbie wymiarów, w których to będą liniowo separowalne. Po przeanalizowaniu dostępnych przypadków wybrano funkcje `RBF`(radial basis function).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "svm_svc = svm.SVC(C=1.1, random_state=1)\n",
    "svm_svc.fit(x_train, y_train)\n",
    "\n",
    "train_labels_predicted = svm_svc.predict(x_train)\n",
    "test_labels_predicted = svm_svc.predict(x_test)\n",
    "\n",
    "\n",
    "print('SVM percentage correct for test data: ', 100*accuracy_score(train_labels_predicted, y_train))\n",
    "print(classification_report(y_train, train_labels_predicted))\n",
    "print(confusion_matrix(y_train, train_labels_predicted))\n",
    "\n",
    "\n",
    "print('SVM percentage correct for test data: ', 100*accuracy_score(test_labels_predicted, y_test))\n",
    "print(classification_report(y_test, test_labels_predicted))\n",
    "print(confusion_matrix(y_test, test_labels_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyniki Algorytmu SVM\n",
    "\n",
    "Jak można zaobserwować algorytm dla zbioru uczącego ma dokładność na poziomie 100 procent. Dla zbioru testowego wydajność jest już gorsza natomiast jest powyżej 84 procent co do daje względnie dużą dokładność. Najgorsze wyniki klasyfikacji zostały otrzymane dla konia natomiast najlepsze dla kurczaka oraz pająka.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Krzywa uczenia algorytmów ucznia maszynowego\n",
    "\n",
    "Jednym z elementów pomocniczych w trakcie wykorzystywania algorytmów ucznia maszynowego jest krzywa uczenia. \n",
    "Przez analizę krzywej uczenia można obserwować, czy model nie został przetrenowany, jak skaluje się model\n",
    "oraz jakie wyniki uzyskuje. Poniżej zostało zaimplementowane wyświetlanie krzywych ucznie z wykorzystaniem \n",
    "walidacji krzyżowej.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    X,\n",
    "    y,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "):\n",
    "    \n",
    "    _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title('Learning curves')\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "   \n",
    "    # Plot learning curvea\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.2, random_state=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poniżej zostały przedstawione krzywe uczenia dla algorytmu SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(svm_svc, hog_train, y, cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie perceptronu wielowarstwowego(MLP) w zadaniu klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_classifier = MLPClassifier(alpha=1, max_iter=1000)\n",
    "mlp_classifier.fit(x_train, y_train)\n",
    "\n",
    "labels_predicted_mlp = mlp_classifier.predict(x_test)\n",
    "\n",
    "print('MLP percentage correct: ', 100*accuracy_score(labels_predicted_mlp, y_test))\n",
    "print(classification_report(y_test, labels_predicted_mlp))\n",
    "print(confusion_matrix(y_test, labels_predicted_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poniżej zostały przedstawione krzywe uczenia dla algorytmu MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(MLPClassifier(alpha=1, max_iter=1000), hog_train, y, cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie k najbliższych sąsiadów(KNN) w zadaniu klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "neigh.fit(x_train, y_train)\n",
    "\n",
    "labels_predicted_neigh = neigh.predict(x_test)\n",
    "print('Knn percentage correct: ', 100*accuracy_score(labels_predicted_neigh, y_test))\n",
    "print(classification_report(y_test, labels_predicted_neigh))\n",
    "print(confusion_matrix(y_test, labels_predicted_neigh))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poniżej zostały przedstawione krzywe uczenia dla algorytmu KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(neigh, hog_train, y, cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniki łączeni algorytmów uczenia maszynowego \n",
    "\n",
    "### Głosowanie ze względu na większość głosów "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3dbd5569ae6dcde62f5393d08bfc960a02a16ad47201cf041d0382688b60e64"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ml_project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
